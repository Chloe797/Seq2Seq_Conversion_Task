{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5 Submission: Langugae Modelling with a Sequence 2 Sequence Model\n",
        "This notebbok contains the code used to convert the original code given from Machine Translation (MT) to Language Modeling (LM) and tests the trained model for  next word prediction. The results indicate that the model struggles in this area, likely due to the limited training data.\n",
        "Any changes from the original codebase will be outlined throughout the notebook.\n",
        "\n",
        "The main changes stemmed from the fact that the code designed from MT consisted of source and target pairs. Language Modeling meant that the input, output pairs are essentially a slice of sentence from the dataset. This simplified the tokenization and vocabulary processes as it is just for one language. Spacy was kept for tokenizing the English datasets. As much as possible, this code was faithful to the original machine translation implementation. One key difference from the original code was the dataset used. This is discussed in further deatil at the Dataset step."
      ],
      "metadata": {
        "id": "xlrqk9gDyGFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "No changes were needed from the original assignment with the execption that the German Spacy tokenizer was no longer needed."
      ],
      "metadata": {
        "id": "x1X6sXJOvnMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-84kvxrMgLAa"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker\n",
        "# Import the clear_output function from IPython.display module for clearing Jupyter Notebook cell output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# This command installs or upgrades the TorchData library, which provides utilities and abstractions for handling datasets in PyTorch, including data loading and preprocessing.\n",
        "!pip install -U torchdata\n",
        "# This command installs or upgrades the spaCy library, which is a powerful natural language processing (NLP) library in Python. SpaCy provides various functionalities such as tokenization, POS tagging, dependency parsing, and named entity recognition (NER).\n",
        "!pip install -U spacy\n",
        "# This command downloads the English language model 'en_core_web_sm' provided by spaCy. This model includes pre-trained word vectors and various linguistic annotations, making it suitable for tasks such as tokenization, part-of-speech tagging, and syntactic parsing.\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install torchtext==0.15.1\n",
        "# This command clears the output of the current cell in a Jupyter notebook environment. It can be useful for removing clutter and focusing on the most recent output or results.\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the main PyTorch library\n",
        "import torch\n",
        "\n",
        "# Import the torch.nn module, which contains neural network-related classes and functions\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import the torch.optim module, which provides optimization algorithms for updating neural network parameters\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import the DataLoader class from torch.utils.data module, used for loading datasets in mini-batches during training\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Import the pad_sequence function from torch.nn.utils.rnn module, used for padding sequences within a batch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Import the get_tokenizer function from torchtext.data.utils module, used for tokenizing text data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Import the build_vocab_from_iterator function from torchtext.vocab module, used for building vocabulary from tokenized text data\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Import the Iterable and List classes/types from typing module for type hinting\n",
        "from typing import Iterable, List\n",
        "\n",
        "# Import the spaCy library for natural language processing tasks\n",
        "import spacy\n",
        "\n",
        "# Import the NumPy library with the alias np for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import the random module for generating random numbers\n",
        "import random\n",
        "\n",
        "# Import the math module for mathematical functions and constants\n",
        "import math\n",
        "\n",
        "# Import the time module for working with time\n",
        "import time\n",
        "\n"
      ],
      "metadata": {
        "id": "pYXvwh9XgPT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU acceleration) is available on the system\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Execute the shell command to display GPU information using nvidia-smi\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmrp8lWAvXUb",
        "outputId": "80033cb7-4b37-4c66-c065-920420fad6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 23:06:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-d32e78dc81fc>\", line 2, in <cell line: 0>\n",
            "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "<ipython-input-3-d32e78dc81fc>:2: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling\n",
        "\n",
        "The following three cells contain the model codebase which remains unchanged (as requested) from the original assignment."
      ],
      "metadata": {
        "id": "xNQd_a9ivp7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since nn.Module has the backward function built-in, we can implicitly utilise it by inheriting nn.Module.\n",
        "# eliminating the need to implement anything other than the forward pass.\n",
        "class LSTMLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        '''\n",
        "          - Args:\n",
        "                - input_size: The number of expected features in the input.\n",
        "                - hidden_size: The number of features in the hidden state.\n",
        "          - Functionality:\n",
        "                - Initialises the LSTM layer with the specified input size, and hidden size.\n",
        "                - Initialises weight and bias parameters.\n",
        "        '''\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define parameters for a single LSTM layer\n",
        "        # We mark the weight matrices as a parameter of the model using nn.Parameter\n",
        "        # This is done to indicate that this tensor should be considered a model parameter.\n",
        "        # So that backward function in pytorch consider these matrices during optimisation,\n",
        "        # and to calculate the gradients with respect to them.\n",
        "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_g = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_g = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    def forward(self, input, h_prev, c_prev):\n",
        "        # Concatenate input and previous hidden state\n",
        "        # Forget gate\n",
        "        # input: (batch_size, input_size) @ W_f: (input_size, hidden_size) -> (batch_size, hidden_size)\n",
        "        # h_prev: (batch_size, hidden_size) @ U_f: (hidden_size, hidden_size) -> (batch_size, hidden_size)\n",
        "        f = torch.sigmoid(input @ self.W_f + h_prev @ self.U_f + self.b_f)\n",
        "\n",
        "        k = f * c_prev #mask\n",
        "\n",
        "\n",
        "        # Input gate (Add gate)\n",
        "        i = torch.sigmoid(input @ self.W_i + h_prev @ self.U_i + self.b_i)\n",
        "\n",
        "\n",
        "        g = torch.tanh(input @ self.W_g + h_prev @ self.U_g + self.b_g) # Candidate cell state\n",
        "        j = i * g #mask\n",
        "\n",
        "\n",
        "        # Output gate\n",
        "        o = torch.sigmoid(input @ self.W_o + h_prev @ self.U_o + self.b_o)\n",
        "\n",
        "        # Update cell state\n",
        "        c_next = k + j\n",
        "\n",
        "        # Update hidden state\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "class StackLSTMLayers(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        '''\n",
        "          - Args:\n",
        "                - input_size: The number of expected features in the input.\n",
        "                - hidden_size: The number of features in the hidden state.\n",
        "                - num_layers: Number of Stacked recurrent layers.\n",
        "          - Functionality:\n",
        "                - Initialises the LSTM layer with the specified input size, hidden size, and number of layers.\n",
        "                - Initialises weight and bias parameters for each layer.\n",
        "        '''\n",
        "        super(StackLSTMLayers, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Create stack of LSTM layers if num_layers > 1\n",
        "        self.layers = nn.ModuleList([LSTMLayer(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        if hidden is None:\n",
        "            # initialise hidden and cell states if not provided\n",
        "            hidden = self.init_hidden(input.size(1))\n",
        "\n",
        "        # Unpack hidden states\n",
        "        hiddens, cells = hidden\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        # Iterate through each time step\n",
        "        for input_t in input:\n",
        "            # Iterate through each layer\n",
        "            for layer_idx, layer in enumerate(self.layers):\n",
        "                # Pass input through the current layer\n",
        "                hiddens[layer_idx], cells[layer_idx] = layer(input_t, hiddens[layer_idx], cells[layer_idx])\n",
        "\n",
        "                # Update input for the next layer (if any)\n",
        "                if layer_idx < self.num_layers - 1:\n",
        "                    input_t = hiddens[layer_idx]\n",
        "\n",
        "            # Append output of current time step\n",
        "            outputs.append(hiddens[-1])\n",
        "\n",
        "        # Stack outputs along the sequence dimension\n",
        "        outputs = torch.stack(outputs, dim=0)\n",
        "\n",
        "\n",
        "        # Return outputs, hidden states, and cell states\n",
        "        return outputs, (hiddens, cells)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # initialise hidden and cell states for each layer\n",
        "        hiddens = [torch.zeros(batch_size, self.hidden_size, device=device) for _ in range(self.num_layers)]\n",
        "        cells = [torch.zeros(batch_size, self.hidden_size, device=device) for _ in range(self.num_layers)]\n",
        "        return hiddens, cells"
      ],
      "metadata": {
        "id": "PZF-5gvsvkbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = StackLSTMLayers(emb_dim, hid_dim, n_layers)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        #src = [src len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = StackLSTMLayers(emb_dim, hid_dim, n_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "ZeDmIbO01JFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Language modeling using an encoder-decoder architecture.\n",
        "\n",
        "        src: [seq_len, batch_size]  -> Encoder input (full sentence)\n",
        "        trg: [seq_len, batch_size]  -> Decoder input (shifted sequence starting with <sos>)\n",
        "        teacher_forcing_ratio: Probability of using the true previous word instead of the predicted one\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        seq_len = trg.shape[0]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(seq_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        # Pass the input sequence through the encoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is always the <sos> token from trg\n",
        "        input = trg[0, :]\n",
        "\n",
        "        # Loop over sequence length to predict each next word\n",
        "        for t in range(1, seq_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the most likely predicted word\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # Use ground truth next word (if teacher forcing), else use predicted word\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "aBeIVCL0vr5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Dataset\n",
        "\n",
        "The biggest chnages to convert rfom MT to LM come with the dataset used (i.e. monolingual data vs source, target pairs) and the preprocessing of the data.\n",
        "\n",
        "The text data was sourced from the Gutenberg Project which provides text files for many great literary works and can be sourced from the [Gutenberg Project Website](https://www.gutenberg.org/ebooks/31100) [1]. The works of author Jane Austen were chosen as there is numerous amounts of text data available to train, validate and test a model on."
      ],
      "metadata": {
        "id": "Z9Gh0Zutv4XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file location - uploaded here to Google collab. This needs to be done to replicate or mount google drive.\n",
        "DATA_DIR = \"/content/\"\n",
        "\n",
        "\n",
        "# define custom dataset for this task\n",
        "class JaneAustenDataset(Dataset):\n",
        "  def __init__(self, file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      self.lines = f.readlines() #may create unwanted empty spaces\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.lines[idx].strip() #applied .strip to get rid of unwanted trailing whitespaces etc.\n",
        "    return text\n",
        "\n",
        "#loading files and creating dataset - split beforehand for convenience\n",
        "train_dataset = JaneAustenDataset(DATA_DIR + \"jane-austen-text-train.txt\")\n",
        "val_dataset = JaneAustenDataset(DATA_DIR + \"jane-austen-text-val.txt\")\n",
        "test_dataset = JaneAustenDataset(DATA_DIR + \"jane-austen-text-test.txt\")"
      ],
      "metadata": {
        "id": "Ssw_v4mkvtxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Initial Preprocessing##\n",
        "\n",
        "The full text has nearly 4.5 million characters which was too large to run on Google Colab. Therefore the first two books, Persuasion and Northanger Abbey, were chosen and have just over 900,000 characters. For convenience, this was manually split into train, validate and test sets, with a 80\\%, 10\\% and 10\\% split ratio.\n",
        "Undesired text, such as Gutenberg Legal notes were manually removed. Including this text should be explored in the future as it may act as a generalization or regularization text as it very different in style to the works of Jane Austen. It should be noted that there may however, remain other notes in the final text. Examples of the removed text are included in the Appendix."
      ],
      "metadata": {
        "id": "rTB3cSw5zezr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the dataset ###\n",
        "Testing a few cases to check the data was loaded correctly:\n",
        "*  train_dataset[0] -> 'PERSUASION'\n",
        "*  train_dataset[10] -> 'Chapter 1'\n",
        "*  train_dataset[100] -> 'influence had always been great, and they had gone on together most'"
      ],
      "metadata": {
        "id": "f2O9E7oExY4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z1IMtL95w9cG",
        "outputId": "9d33899f-fbb4-4b48-b0a4-88edada663e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PERSUASION'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization & Vocabulary Creation\n",
        "It was decided to keep the same tokenizer as given in the original, with the exception that now only a English Language tokenizer is needed. While keeping the processing as close to the original assignment, it was also desired to make the process more modular and adaptable [2]. The special characters were kept the same format as in the original assignemnt.\n",
        "\n",
        "The vocabulary was optionally saved for future use."
      ],
      "metadata": {
        "id": "LL1PbBQcnMJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer from Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#tokenize text using spacy tokenizer\n",
        "def tokenizer(text):\n",
        "  return [token.text for token in nlp(text)]\n",
        "\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "#combine tokenization process and vocabulary creation by callilng tokenizer\n",
        "def vocabulary(train_iter, vocab_path='vocab.pt', min_freq=2):\n",
        "  \"\"\"\n",
        "  Tokenizes and creates a vocabulary from the training data. Saves Voacb for potential future use\n",
        "\n",
        "  Args:\n",
        "    train_iter(iterator): Iterates the training data\n",
        "    vocab_path(str): optional path to save the vocabulary file for future use\n",
        "    min_freq(int): optional parameter to exclude words below set threshold\n",
        "\n",
        "  Returns:\n",
        "\n",
        "  \"\"\"\n",
        "  #creating vocabulary\n",
        "  vocab = build_vocab_from_iterator((tokenizer(text) for text in train_dataset), specials=special_symbols, min_freq=min_freq)\n",
        "\n",
        "\n",
        "  #set deafult to unk\n",
        "  vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "  #save the vocabulary - optional\n",
        "  torch.save(vocab, vocab_path) #probably could drop this for this assignment\n",
        "\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "0bnC9V0jx1wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vocabulary(train_dataset)"
      ],
      "metadata": {
        "id": "JHPUwv99ySAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yX8b6ExyVBC",
        "outputId": "5cca9428-9226-425a-871a-c98ac941c4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4430"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer and Collate Function\n",
        "These funtions were kept largely the same, except that instead of source,target pairs from two lanaguges there is (input,output) pairs where the output is the next word of the input sentence."
      ],
      "metadata": {
        "id": "9xDpK1-A7LmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert text into tokens and add special characters\n",
        "def tensor_transform(text):\n",
        "  tokens = tokenizer(text)\n",
        "  token_ids = [vocab[token] if token in vocab else UNK_IDX for token in tokens]\n",
        "  #concatonates the sentence with the sos and eos tags in return\n",
        "  return torch.cat((torch.tensor([BOS_IDX], dtype=torch.long),\n",
        "                    torch.tensor(token_ids, dtype=torch.long),\n",
        "                    torch.tensor([EOS_IDX], dtype=torch.long)))"
      ],
      "metadata": {
        "id": "XlRB-Kg57UVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "  input_batch, output_batch = [], []\n",
        "\n",
        "  for sample in batch:\n",
        "    token_ids = tensor_transform(sample.rstrip(\"\\n\")) #removes newlines at end\n",
        "\n",
        "    input_batch.append(token_ids[:-1]) #w1,w2,w3,w4\n",
        "    output_batch.append(token_ids[1:]) #w5\n",
        "\n",
        "  #add padding term to create equal length sequences - moved outside the loop\n",
        "  #length will deafult be longest sentence in batch\n",
        "  input_batch = pad_sequence(input_batch, padding_value=PAD_IDX, batch_first = False) #batch_first set for LSTM dimensions\n",
        "  output_batch = pad_sequence(output_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "  return input_batch, output_batch"
      ],
      "metadata": {
        "id": "TDuQuCYKynoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "T9Kh2DFkDLSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Setup\n",
        "sample_input, sample_output = next(iter(train_loader))\n",
        "\n",
        "#Checking Tensor shapes\n",
        "print(\"Input Shape:\", sample_input.shape)  #(seq_len, batch_size)\n",
        "print(\"Output batch shape:\", sample_output.shape)  #(seq_len, batch_size)\n",
        "\n",
        "# Debugging: Print first tokenized sequence\n",
        "print(\"\\nFirst input sequence (token IDs):\", sample_input[:, 0])  # Checking first sentence\n",
        "print(\"First output sequence (token IDs):\", sample_output[:, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KUf6I6OztCq",
        "outputId": "6ee93982-eccd-4448-e090-9e9ea64fa352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([23, 32])\n",
            "Output batch shape: torch.Size([23, 32])\n",
            "\n",
            "First input sequence (token IDs): tensor([2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "First output sequence (token IDs): tensor([0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total dataset size:\", len(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOXZEdTM0dip",
        "outputId": "de6a24a2-1d60-44a4-d415-67b94118d811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 12922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Model\n",
        "Initialization was kept mostly the same, however, there is now only one vocabulary now which simplifies the dimensions."
      ],
      "metadata": {
        "id": "OgVvALPu00Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "jfdi8qgc0xQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df664f1-33a1-40cc-91dd-e2c85b034558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 11,888,974 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate\n",
        "This code is largely unchanged from the original assignment. The training takes approximately 30 minutes to run."
      ],
      "metadata": {
        "id": "z-Hpjl3nqFvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    # iterating over dataloader batches\n",
        "    for batch in dataloader:\n",
        "\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(inputs, targets) #added targets here\n",
        "\n",
        "        output = output.view(-1, output.size(-1)) #flatten for loss calculation\n",
        "        targets = targets.view(-1).to(device)\n",
        "\n",
        "        loss = criterion(output, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in dataloader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "            output = model(inputs, targets, 0) #0 == turn off teacher forcing\n",
        "\n",
        "\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            targets = targets.view(-1).to(device)\n",
        "\n",
        "\n",
        "            loss = criterion(output, targets) #calculating loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "8p5MzILO1b5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion, device)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lstm-seq2seq-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbFlBuix1jF5",
        "outputId": "5ed1ba54-e7d9-4a9a-f56a-ed49fe5cc5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 28s\n",
            "\tTrain Loss: 5.902 | Train PPL: 365.928\n",
            "\t Val. Loss: 5.888 |  Val. PPL: 360.659\n",
            "\t Test. Loss: 5.890 |  Test. PPL: 361.546\n",
            "Epoch: 02 | Time: 2m 26s\n",
            "\tTrain Loss: 5.421 | Train PPL: 226.183\n",
            "\t Val. Loss: 5.580 |  Val. PPL: 265.192\n",
            "\t Test. Loss: 5.584 |  Test. PPL: 266.136\n",
            "Epoch: 03 | Time: 2m 26s\n",
            "\tTrain Loss: 5.106 | Train PPL: 164.969\n",
            "\t Val. Loss: 5.439 |  Val. PPL: 230.231\n",
            "\t Test. Loss: 5.448 |  Test. PPL: 232.286\n",
            "Epoch: 04 | Time: 2m 25s\n",
            "\tTrain Loss: 4.703 | Train PPL: 110.260\n",
            "\t Val. Loss: 4.988 |  Val. PPL: 146.680\n",
            "\t Test. Loss: 5.002 |  Test. PPL: 148.752\n",
            "Epoch: 05 | Time: 2m 25s\n",
            "\tTrain Loss: 4.344 | Train PPL:  76.984\n",
            "\t Val. Loss: 4.738 |  Val. PPL: 114.186\n",
            "\t Test. Loss: 4.757 |  Test. PPL: 116.345\n",
            "Epoch: 06 | Time: 2m 26s\n",
            "\tTrain Loss: 4.049 | Train PPL:  57.331\n",
            "\t Val. Loss: 4.593 |  Val. PPL:  98.819\n",
            "\t Test. Loss: 4.595 |  Test. PPL:  99.025\n",
            "Epoch: 07 | Time: 2m 23s\n",
            "\tTrain Loss: 3.793 | Train PPL:  44.407\n",
            "\t Val. Loss: 4.465 |  Val. PPL:  86.964\n",
            "\t Test. Loss: 4.448 |  Test. PPL:  85.445\n",
            "Epoch: 08 | Time: 2m 24s\n",
            "\tTrain Loss: 3.557 | Train PPL:  35.056\n",
            "\t Val. Loss: 4.325 |  Val. PPL:  75.602\n",
            "\t Test. Loss: 4.311 |  Test. PPL:  74.531\n",
            "Epoch: 09 | Time: 2m 23s\n",
            "\tTrain Loss: 3.316 | Train PPL:  27.551\n",
            "\t Val. Loss: 4.212 |  Val. PPL:  67.498\n",
            "\t Test. Loss: 4.193 |  Test. PPL:  66.201\n",
            "Epoch: 10 | Time: 2m 27s\n",
            "\tTrain Loss: 3.081 | Train PPL:  21.784\n",
            "\t Val. Loss: 4.164 |  Val. PPL:  64.317\n",
            "\t Test. Loss: 4.153 |  Test. PPL:  63.649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Testing on 10 sentences\n",
        "\n",
        "It was decided that the ten test sentences should be taken from a range of time to test how well the model could generalize.\n",
        "The test sentences are mainly a range of famous quotes from famous books.\n",
        "The quotes chosen are as follows:\n",
        "\n",
        "*  Jane Austen, Pride and Prejudice\n",
        "*  Jane Austen, Sense and Sensibility\n",
        "*  Charles Dickens, A Tale of two Cities\n",
        "*  Charlotte Brontë, Jane Eyre\n",
        "*  Test Sentence given in Assignment\n",
        "\n",
        "This gives a broad range of test sentences, some are in domain and should predict well such (1-5) as they are from the same author and should be similar in style. The sentences 6-9 are from authors just after Jane Austen's time and so therefore there may be stylistic similarities but also differences. The last sentence is a modern sentence and it is not expected to predict well on this sentence.\n",
        "\n",
        "The last word was removed from each sentence to give the final test dataset. A generate function was added so that the model would choose the most likely next word of a given sentence based on its training. The results are included in the table below.\n",
        "\n",
        "This [resource](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) [3] which has a generation function for MT was adapted for this task.\n",
        "\n",
        "**Note**: This notebook can either be run in full or the model path has been saved and shared and can be uploaded and loaded here using\n",
        "```\n",
        "model.load_state_dict(torch.load(\"lstm-seq2seq-model.pt\"))\n",
        "```"
      ],
      "metadata": {
        "id": "mmV_JVpKE_eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_word(model, sentence, vocab, device):\n",
        "  model.eval() # set model to evaluation mode\n",
        "\n",
        "  #Use tensor_transform which processes token for model\n",
        "  input_tensor = tensor_transform(sentence).unsqueeze(1).to(device)\n",
        "  #encoding\n",
        "  hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "  #decoding/generation\n",
        "  input_word = input_tensor[-1, 0].unsqueeze(0) #i.e. start at last word of the given sentence\n",
        "  with torch.no_grad():\n",
        "    output, hidden, cell = model.decoder(input_word, hidden, cell)\n",
        "    #get probability distribution for next word\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    #choose top word\n",
        "    top_word = torch.argmax(probs)\n",
        "    next_word = vocab.lookup_token(top_word.item())\n",
        "\n",
        "  return next_word"
      ],
      "metadata": {
        "id": "wC9osat6BZ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing ten sentences\n",
        "test_sentences = [\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a\",\n",
        "                     \"Do anything rather than marry without\",\n",
        "                     \"There are very few who have heart enough to be really in love without\",\n",
        "                     \"The more I know of the world, the more I am convinced that I shall never see a man whom I can really love. I require so\",\n",
        "                     \"Seven years would be insufficient to make some people acquainted with each other, and seven days are more than enough for\",\n",
        "                     \"You have been the last dream of my\",\n",
        "                     \"It was the best of times, it was the worst of\",\n",
        "                     \"I am no bird; and no net ensnares me: I am a free human being with an independent\",\n",
        "                     \"I would always rather be happy than\",\n",
        "                     \"I love Neural\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "  next_word = generate_next_word(model, sentence, vocab, device)\n",
        "  print(f\"Input: {sentence}\")\n",
        "  print(f\"Predicted Next Word: {sentence} + {next_word}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i7kGF1My_pJ",
        "outputId": "5d9d6d46-7013-49a1-81e9-50b371cd57fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a\n",
            "Predicted Next Word: It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a + is\n",
            "\n",
            "Input: Do anything rather than marry without\n",
            "Predicted Next Word: Do anything rather than marry without + for\n",
            "\n",
            "Input: There are very few who have heart enough to be really in love without\n",
            "Predicted Next Word: There are very few who have heart enough to be really in love without + \"\n",
            "\n",
            "Input: The more I know of the world, the more I am convinced that I shall never see a man whom I can really love. I require so\n",
            "Predicted Next Word: The more I know of the world, the more I am convinced that I shall never see a man whom I can really love. I require so + at\n",
            "\n",
            "Input: Seven years would be insufficient to make some people acquainted with each other, and seven days are more than enough for\n",
            "Predicted Next Word: Seven years would be insufficient to make some people acquainted with each other, and seven days are more than enough for + seemed\n",
            "\n",
            "Input: You have been the last dream of my\n",
            "Predicted Next Word: You have been the last dream of my + is\n",
            "\n",
            "Input: It was the best of times, it was the worst of\n",
            "Predicted Next Word: It was the best of times, it was the worst of + was\n",
            "\n",
            "Input: I am no bird; and no net ensnares me: I am a free human being with an independent\n",
            "Predicted Next Word: I am no bird; and no net ensnares me: I am a free human being with an independent + he\n",
            "\n",
            "Input: I would always rather be happy than\n",
            "Predicted Next Word: I would always rather be happy than + to\n",
            "\n",
            "Input: I love Neural\n",
            "Predicted Next Word: I love Neural + of\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results & Discussion\n",
        "\n",
        "While the losses are still relatively high, it can be observed that with each epoch, the loss was dropping and that for most epochs, the Test Loss was the highest, followed by Validation Loss and then Train Loss, which is to be expected. The number of epochs was selected in order to avoid extremely long run times, especially with limited T4 GPU usage allowed by Google Colab.\n",
        "\n",
        "It can be seen that these results are suboptimal, with none being accurately predicted.  Only sentence 9 exhibited 'local' coherency, with 'than to' being an old fashioned but grammatical piece of text.  For example, *I would rather be X than to be Y*. This dated construction makes sense given the historical style of writing on which the model was trained.\n",
        "However, this is just locally coherent and none of the sentences make sense overall. This is likely due to the limited amount of text the model was trained on. As mentioned, the model path has been saved and is shared for further testing.\n"
      ],
      "metadata": {
        "id": "pRXTnLHA3yJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Evaluation of Model\n",
        "\n",
        "Automatic metrics, namely BLEU and BERTScore were implemented to evaluate the generated text.\n",
        "Due to computation limitations, the evaluation was carried out in a separate notebook that has been shared along with this notebook called 'Evaluation of LM Text'.ipynb. For convenience the contents of that notebook are included below, with the code in markdown for future use.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Installing required packages\n",
        "!pip install nltk #to get sentence bleu\n",
        "!pip install evaluate #to load in bertscore\n",
        "!pip install bert_score #bertscore\n",
        "\n",
        "# Imports needed\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "from  evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "```\n",
        "# Full Sentence evaluation\n",
        "## Data\n",
        "The data below consists of two lists:\n",
        "* The actual famous quotes.\n",
        "* The sentences whose last words were predicted using the model trained for the assignment.\n",
        "\n",
        "We will now use to metrics to evaluate these sentences:\n",
        "1.  BLEU Score [4] [5]\n",
        "2.  BERT Score [6] [7]\n",
        "\n",
        "```\n",
        "# Real quotes\n",
        "reference_sentences = [\n",
        "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife\",\n",
        "    \"Do anything rather than marry without affection\",\n",
        "    \"There are very few who have heart enough to be really in love without encouragement\",\n",
        "    \"The more I know of the world, the more I am convinced that I shall never see a man whom I can really love. I require so much\",\n",
        "    \"Seven years would be insufficient to make some people acquainted with each other, and seven days are more than enough for others\",\n",
        "    \"You have been the last dream of my soul\",\n",
        "    \"It was the best of times, it was the worst of times\",\n",
        "    \"I am no bird; and no net ensnares me: I am a free human being with an independent will\",\n",
        "    \"I would always rather be happy than dignified\",\n",
        "    \"I love Neural Networks\"\n",
        "]\n",
        "\n",
        "#Text Generated by Model\n",
        "generated_sentences = [\n",
        "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a is\",\n",
        "    \"Do anything rather than marry without for\",\n",
        "    \"There are very few who have heart enough to be really in love without ”\",\n",
        "    \"The more I know of the world, the more I am convinced that I shall never see a man whom I can really love. I require so at\",\n",
        "    \"Seven years would be insufficient to make some people acquainted with each other, and seven days are more than enough for seemed\",\n",
        "    \"You have been the last dream of my is\",\n",
        "    \"It was the best of times, it was the worst of was\",\n",
        "    \"I am no bird; and no net ensnares me: I am a free human being with an independent he\",\n",
        "    \"I would always rather be happy than to\",\n",
        "    \"I love Neural of\"\n",
        "]\n",
        "```\n",
        "## BLEU\n",
        "BLEU is a popular evaluation metric for MT tasks and is used here to evaluate the next word prediction task.\n",
        "BLEU compares the n-grams from human sentences to the translated (or in this case generated) sentences.\n",
        "BLEU is implemented on each sentence below using python and this particular BLEU metric is between 0 (no matches) and 1 (all matched) [5].\n",
        "\n",
        "```\n",
        "#can assign weights to put more preferance on lower order n-grams.\n",
        "#As BLEU can decrease with longer sentence\n",
        "weights = (0.25, 0.25, 0.25, 0.75) #gave 4-grams a higher weight to match more of the sentence, i.e. including the generated word more often\n",
        "\n",
        "#bleu requires the sentences to be split into words so the .split() was used. # .zip() creates (ref,gen) tuple\n",
        "bleu_score = [sentence_bleu([reference.split()], generated.split(), weights=weights) for reference, generated in zip(reference_sentences, generated_sentences)]\n",
        "\n",
        "#printing the score\n",
        "for i, score in enumerate(bleu_score):\n",
        "    print(f\"Sentence {i+1} BLEU Score: {score:.2f}\")\n",
        "\n",
        "```\n",
        "\n",
        "Results:\n",
        "*  Sentence 1 BLEU Score: 0.93\n",
        "*  Sentence 2 BLEU Score: 0.70\n",
        "*  Sentence 3 BLEU Score: 0.89\n",
        "*  Sentence 4 BLEU Score: 0.94\n",
        "*  Sentence 5 BLEU Score: 0.93\n",
        "*  Sentence 6 BLEU Score: 0.79\n",
        "*  Sentence 7 BLEU Score: 0.85\n",
        "*  Sentence 8 BLEU Score: 0.91\n",
        "*  Sentence 9 BLEU Score: 0.75\n",
        "*  Sentence 10 BLEU Score: 0.00\n",
        "\n",
        "\n",
        "## BERTScore\n",
        "\n",
        "BERTScore uses pre-trained contextual embeddings (BERT) and uses cosine similarity to match words in generated and reference sentences. The score is between 0 (no semantic similarity) and 1 (identical). [8]\n",
        "\n",
        "```\n",
        "#computing sentence level semantic similarity\n",
        "bert_score = bertscore.compute(predictions=generated_sentences, references=reference_sentences, lang=\"en\")\n",
        "\n",
        "#printing results\n",
        "for i in range(len(generated_sentences)):\n",
        "  print(f\"Sentence {i+1} BERTScore:\")\n",
        "  print(f\"  Precision: {bert_score['precision'][i]:.2f}\")\n",
        "  print(f\"  Recall: {bert_score['recall'][i]:.2f}\")\n",
        "  print(f\"  F1: {bert_score['f1'][i]:.2f}\")\n",
        "```\n",
        "\n",
        "Results:\n",
        "*  Sentence 1 BERTScore:\n",
        "  *  Precision: 0.96\n",
        "  *  Recall: 0.96\n",
        "  *  F1: 0.96\n",
        "*  Sentence 2 BERTScore:\n",
        "  *  Precision: 0.96\n",
        "  *  Recall: 0.96\n",
        "  *  F1: 0.96\n",
        "*  Sentence 3 BERTScore:\n",
        "  *  Precision: 0.96\n",
        "  *  Recall: 0.97\n",
        "  *  F1: 0.96\n",
        "*  Sentence 4 BERTScore:\n",
        "  *  Precision: 0.99\n",
        "  *  Recall: 0.99\n",
        "  *  F1: 0.99\n",
        "*  Sentence 5 BERTScore:\n",
        "  *  Precision: 0.97\n",
        "  *  Recall: 0.98\n",
        "  *  F1: 0.97\n",
        "*  Sentence 6 BERTScore:\n",
        "  *  Precision: 0.96\n",
        "  *  Recall: 0.96\n",
        "  *  F1: 0.96\n",
        "*  Sentence 7 BERTScore:\n",
        "  *  Precision: 0.96\n",
        "  *  Recall: 0.97\n",
        "  *  F1: 0.97\n",
        "*  Sentence 8 BERTScore:\n",
        "  *  Precision: 0.98\n",
        "  *  Recall: 0.98\n",
        "  *  F1: 0.98\n",
        "*  Sentence 9 BERTScore:\n",
        "  *  Precision: 0.97\n",
        "  *  Recall: 0.95\n",
        "  *  F1: 0.96\n",
        "*  Sentence 10 BERTScore:\n",
        "  *  Precision: 0.89\n",
        "  *  Recall: 0.90\n",
        "  *  F1: 0.89\n",
        "\n",
        "\n",
        "## Sentence - Level Results:\n",
        "As we can see, both BLEU and BERTScore return very high scores. The BLEU score was adjusted to have a higher weight towards 4-grams. As can be seen, this casused the BLEU score for the last sentence to be zero, which makes sense as the sentence has four words with one mismatch. However, this is due to the fact that the sentences are by the nature of the model, identical until the last word. To create a more accurate experiment, bigrams of the last two words from each sentence were taken and BLEU and BERTScore were obtained for these bigrams.\n",
        "\n",
        "## Bigram study\n",
        "```\n",
        "# Real quotes\n",
        "reference_bigrams = [\n",
        "    \"a wife\",\n",
        "    \"without affection\",\n",
        "    \"without encouragement\",\n",
        "    \"so much\",\n",
        "    \"for others\",\n",
        "    \"my soul\",\n",
        "    \"of times\",\n",
        "    \"independent will\",\n",
        "    \"than dignified\",\n",
        "    \"Neural Networks\"\n",
        "]\n",
        "\n",
        "#Text Generated by Model\n",
        "generated_bigrams = [\n",
        "    \"a is\",\n",
        "    \"without for\",\n",
        "    \"without ”\",\n",
        "    \"so at\",\n",
        "    \"for seemed\",\n",
        "    \"my is\",\n",
        "    \"of was\",\n",
        "    \"independent he\",\n",
        "    \"than to\",\n",
        "    \"Neural of\"\n",
        "]\n",
        "\n",
        "#BLEU\n",
        "\n",
        "weights_2 = (1,0,0,0)#i.e. only bigrams\n",
        "bleu_bigrams = [sentence_bleu([reference.split()], generated.split(), weights=weights_2) for reference, generated in zip(reference_bigrams, generated_bigrams)]\n",
        "\n",
        "#printing results\n",
        "for i, score in enumerate(bleu_bigrams):\n",
        "    print(f\"Sentence {i+1} BLEU Score: {score:.2f}\")\n",
        "```\n",
        "\n",
        "Results:\n",
        "*  Sentence 1 BLEU Score: 0.50\n",
        "*  Sentence 2 BLEU Score: 0.50\n",
        "*  Sentence 3 BLEU Score: 0.50\n",
        "*  Sentence 4 BLEU Score: 0.50\n",
        "*  Sentence 5 BLEU Score: 0.50\n",
        "*  Sentence 6 BLEU Score: 0.50\n",
        "*  Sentence 7 BLEU Score: 0.50\n",
        "*  Sentence 8 BLEU Score: 0.50\n",
        "*  Sentence 9 BLEU Score: 0.50\n",
        "*  Sentence 10 BLEU Score: 0.50\n",
        "\n",
        "```\n",
        "#BERTScore\n",
        "bert_score_bigrams = bertscore.compute(predictions=generated_bigrams, references=reference_bigrams, lang=\"en\")\n",
        "\n",
        "#printing results\n",
        "for i in range(len(generated_sentences)):\n",
        "  print(f\"Sentence {i+1} BERTScore:\")\n",
        "  print(f\"  Precision: {bert_score_bigrams['precision'][i]:.2f}\")\n",
        "  print(f\"  Recall: {bert_score_bigrams['recall'][i]:.2f}\")\n",
        "  print(f\"  F1: {bert_score_bigrams['f1'][i]:.2f}\")\n",
        "```\n",
        "\n",
        "Results:\n",
        "*  Sentence 1 BERTScore:\n",
        "  *  Precision: 0.88\n",
        "  *  Recall: 0.88\n",
        "  *  F1: 0.88\n",
        "*  Sentence 2 BERTScore:\n",
        "  *  Precision: 0.87\n",
        "  *  Recall: 0.88\n",
        "  *  F1: 0.87\n",
        "*  Sentence 3 BERTScore:\n",
        "  *  Precision: 0.80\n",
        "  *  Recall: 0.86\n",
        "  *  F1: 0.83\n",
        "*  Sentence 4 BERTScore:\n",
        "  *  Precision: 0.84\n",
        "  *  Recall: 0.85\n",
        "  *  F1: 0.85\n",
        "*  Sentence 5 BERTScore:\n",
        "  *  Precision: 0.84\n",
        "  *  Recall: 0.87\n",
        "  *  F1: 0.86\n",
        "*  Sentence 6 BERTScore:\n",
        "  *  Precision: 0.85\n",
        "  *  Recall: 0.85\n",
        "  *  F1: 0.85\n",
        "*  Sentence 7 BERTScore:\n",
        "  *  Precision: 0.87\n",
        "  *  Recall: 0.86\n",
        "  *  F1: 0.86\n",
        "*  Sentence 8 BERTScore:\n",
        "  *  Precision: 0.93\n",
        "  *  Recall: 0.92\n",
        "  *  F1: 0.92\n",
        "*  Sentence 9 BERTScore:\n",
        "  *  Precision: 0.88\n",
        "  *  Recall: 0.85\n",
        "  *  F1: 0.87\n",
        "*  Sentence 10 BERTScore:\n",
        "  *  Precision: 0.87\n",
        "  *  Recall: 0.83\n",
        "  *  F1: 0.85\n",
        "\n",
        "## Results from Bigram Study\n",
        "It can be seen here that the BLEU catches that none of the next words from the model were accurately predicted with a score of 0.5 for all. It is surprising that the BERTScore is as high as it is. However, it is likely that the more words generated by the model, the likely the BERTScore is to fall.\n"
      ],
      "metadata": {
        "id": "ot9Em5mv_-yK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Directions ##\n",
        "As mentioned earlier, it was observed that while the Loss was still high in the final epoch, losses were gradually dropping. A future extension may be to increase the number of epochs further and examine whether this results in a improved performance. Another option could be to add more works of Jane Austen or other Authors. However, as previously mentioned, this may need to be done locally, due to Colab usage limitations. The model could be expanded to predicted more words than just the next and automatic metrics, such as BLEU and BERTScore may be suitable evaluation tools for this extended task."
      ],
      "metadata": {
        "id": "jCAS6g2QVVGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "*  [1] Project Gutenberg, “The Complete Project Gutenberg Works of Jane Austen by Jane Austen,” Project Gutenberg, Jan. 25, 2010. Available: https://www.gutenberg.org/ebooks/31100. [Accessed: Mar. 29, 2025]\n",
        "*  [2] Ebimsv, “GitHub - Ebimsv/Torch-Linguist: Language Modeling with PyTorch,” GitHub, 2023. Available: https://github.com/Ebimsv/Torch-Linguist. [Accessed: Mar. 29, 2025]\n",
        "*  [3] bentrevett, “pytorch-seq2seq/1 - Sequence to Sequence Learning with Neural Networks.ipynb at main · bentrevett/pytorch-seq2seq,” GitHub, 2018. Available: https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb. [Accessed: Mar. 29, 2025]\n",
        "*  [4] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a Method for Automatic Evaluation of Machine Translation,” Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - ACL ’02, pp. 311–318, 2001, doi: https://doi.org/10.3115/1073083.1073135. Available: https://dl.acm.org/citation.cfm?id=1073135\n",
        "*  [5] GeeksforGeeks, “NLP BLEU Score for Evaluating Neural Machine Translation Python,” GeeksforGeeks, Oct. 23, 2022. Available: https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/#what-is-bleu-score. [Accessed: Mar. 29, 2025]\n",
        "*  [6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ,” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, vol. 1, pp. 4171–4186, 2019, Available: https://aclanthology.org/N19-1423/. [Accessed: Mar. 29, 2025]\n",
        "*  [7] Tiiiger, “GitHub - Tiiiger/bert_score: BERT score for text generation,” GitHub, Feb. 20, 2023. Available: https://github.com/Tiiiger/bert_score?tab=readme-ov-file#usage. [Accessed: Mar. 29, 2025]\n",
        "*  [8] “BERT Score - a Hugging Face Space by evaluate-metric,” huggingface.co. Available: https://huggingface.co/spaces/evaluate-metric/bertscore. [Accessed: Mar. 29, 2025]"
      ],
      "metadata": {
        "id": "MbFkpZiEABoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix\n",
        "Example of Text Manually Removed\n",
        "````\n",
        "The Project Gutenberg eBook of The Complete Project Gutenberg Works of Jane Austen\n",
        "        \n",
        "    This ebook is for the use of anyone anywhere in the United States and\n",
        "    most other parts of the world at no cost and with almost no restrictions\n",
        "    whatsoever. You may copy it, give it away or re-use it under the terms\n",
        "    of the Project Gutenberg License included with this ebook or online\n",
        "    at www.gutenberg.org. If you are not located in the United States,\n",
        "    you will have to check the laws of the country where you are located\n",
        "    before using this eBook.\n",
        "\n",
        "    Title: The Complete Project Gutenberg Works of Jane Austen\n",
        "\n",
        "    Author: Jane Austen\n",
        "\n",
        "    Editor: David Widger\n",
        "\n",
        "    Release date: January 25, 2010 [eBook \\#31100]\n",
        "                  Most recently updated: June 15, 2013\n",
        "\n",
        "    Language: English\n",
        "\n",
        "    Credits: Produced by David Widger\n",
        "\n",
        "\n",
        "    *** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE PROJECT GUTENBERG WORKS OF JANE AUSTEN ***\n",
        "\n",
        "    Produced by many Project Gutenberg volunteers.\n",
        "\n",
        "\n",
        "    THE WORKS OF JANE AUSTEN\n",
        "\n",
        "\n",
        "    Edited by David Widger\n",
        "\n",
        "    Project Gutenberg Editions\n",
        "\n",
        "\n",
        "\n",
        "                DEDICATION\n",
        "\n",
        "        This Jane Austen collection\n",
        "            is dedicated to\n",
        "        Alice Goodson [Hart] Woodby\n",
        "\n",
        "\n",
        "\n",
        "    [Note: The accompanying HTML file has active links to all the volumes\n",
        "    and chapters in this set.]\n",
        "\n",
        "\n",
        "    CONTENTS:\n",
        "\n",
        "      PERSUASION\n",
        "\n",
        "      NORTHANGER ABBEY\n",
        "\n",
        "      MANSFIELD PARK\n",
        "\n",
        "      EMMA\n",
        "\n",
        "      LADY SUSAN\n",
        "\n",
        "      LOVE AND FREINDSHIP AND OTHER EARLY WORKS\n",
        "\n",
        "      PRIDE AND PREJUDICE\n",
        "\n",
        "      SENSE AND SENSIBILITY\n",
        "````"
      ],
      "metadata": {
        "id": "xYqcoSYxV6D1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGaxF08OAzaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}